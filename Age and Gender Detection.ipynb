{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3cea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b90205",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '../UTKFace/'#set your base directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f47a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "age_labels = []\n",
    "gender_labels = []\n",
    "\n",
    "for filename in tqdm(os.listdir(BASE_DIR)):\n",
    "    image_path = os.path.join(BASE_DIR, filename)\n",
    "    temp = filename.split('_')\n",
    "    age = int(temp[0])\n",
    "    gender = int(temp[1])\n",
    "    image_paths.append(image_path)\n",
    "    age_labels.append(age)\n",
    "    gender_labels.append(gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecdd0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['image'], df['age'], df['gender'] = image_paths, age_labels, gender_labels\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels for gender\n",
    "gender_dict = {0:'Male', 1:'Female'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75e54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "#img = Image.open(df['image'][0])\n",
    "#plt.axis('off')\n",
    "#plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5782c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83ace9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "files = df.iloc[0:25]\n",
    "\n",
    "for index, file, age, gender in files.itertuples():\n",
    "    plt.subplot(5, 5, index+1)\n",
    "    img = load_img(file)\n",
    "    img = np.array(img)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Age: {age} Gender: {gender_dict[gender]}\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ffa981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image, grayscale=True)\n",
    "        img = img.resize((128, 128), Image.ANTIALIAS)\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "        \n",
    "    features = np.array(features)\n",
    "    # ignore this step if using RGB\n",
    "    features = features.reshape(len(features), 128, 128, 1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60611f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = extract_features(df['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255.0 # scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b26e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gender = np.array(df['gender'])\n",
    "y_age = np.array(df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c57798",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (128, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3318e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((input_shape))\n",
    "# convolutional layers\n",
    "conv_1 = Conv2D(32, kernel_size=(3, 3), activation='relu') (inputs)\n",
    "maxp_1 = MaxPooling2D(pool_size=(2, 2)) (conv_1)\n",
    "conv_2 = Conv2D(64, kernel_size=(3, 3), activation='relu') (maxp_1)\n",
    "maxp_2 = MaxPooling2D(pool_size=(2, 2)) (conv_2)\n",
    "conv_3 = Conv2D(128, kernel_size=(3, 3), activation='relu') (maxp_2)\n",
    "maxp_3 = MaxPooling2D(pool_size=(2, 2)) (conv_3)\n",
    "conv_4 = Conv2D(256, kernel_size=(3, 3), activation='relu') (maxp_3)\n",
    "maxp_4 = MaxPooling2D(pool_size=(2, 2)) (conv_4)\n",
    "\n",
    "flatten = Flatten() (maxp_4)\n",
    "\n",
    "# fully connected layers\n",
    "dense_1 = Dense(256, activation='relu') (flatten)\n",
    "dense_2 = Dense(256, activation='relu') (flatten)\n",
    "\n",
    "dropout_1 = Dropout(0.3) (dense_1)\n",
    "dropout_2 = Dropout(0.3) (dense_2)\n",
    "\n",
    "output_1 = Dense(1, activation='sigmoid', name='gender_out') (dropout_1)\n",
    "output_2 = Dense(1, activation='relu', name='age_out') (dropout_2)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[output_1, output_2])\n",
    "\n",
    "model.compile(loss=['binary_crossentropy', 'mae'], optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57676a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=X, y=[y_gender, y_age], batch_size=32, epochs=30, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d8cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender\n",
    "acc = history.history['gender_out_accuracy']\n",
    "val_acc = history.history['val_gender_out_accuracy']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Accuracy Graph')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "loss = history.history['gender_out_loss']\n",
    "val_loss = history.history['val_gender_out_loss']\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Loss Graph')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGE\n",
    "loss = history.history['age_out_loss']\n",
    "val_loss = history.history['val_age_out_loss']\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Loss Graph')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "image_index = 100\n",
    "print(\"Original Gender:\", gender_dict[y_gender[image_index]], \"Original Age:\", y_age[image_index])\n",
    "# predict from model\n",
    "pred = model.predict(X[image_index].reshape(1, 128, 128, 1))\n",
    "pred_gender = gender_dict[round(pred[0][0][0])]\n",
    "pred_age = round(pred[1][0][0])\n",
    "print(\"Predicted Gender:\", pred_gender, \"Predicted Age:\", pred_age)\n",
    "plt.axis('off')\n",
    "plt.imshow(X[image_index].reshape(128, 128), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 10000\n",
    "print(\"Original Gender:\", gender_dict[y_gender[image_index]], \"Original Age:\", y_age[image_index])\n",
    "# predict from model\n",
    "pred = model.predict(X[image_index].reshape(1, 128, 128, 1))\n",
    "pred_gender = gender_dict[round(pred[0][0][0])]\n",
    "pred_age = round(pred[1][0][0])\n",
    "print(\"Predicted Gender:\", pred_gender, \"Predicted Age:\", pred_age)\n",
    "plt.axis('off')\n",
    "plt.imshow(X[image_index].reshape(128, 128), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d3c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb528e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for real time open videocapture loop from open cv and perform resizing and grayscaling inside loop and call model from inside the loop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71645fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
